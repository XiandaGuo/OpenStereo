# Advanced Usages

Nothing here yet.

[//]: # (## Cross-Dataset Evalution)
[//]: # (### Cross-Dataset Evalution)

[//]: # (> You can conduct cross-dataset evalution by just modifying several arguments in your [data_cfg]&#40;../configs/psmnet/baseline.yaml#L1&#41;.)

[//]: # (>)

[//]: # (>  Take [baseline.yaml]&#40;../configs/psmnet/baseline.yaml&#41; as an example:)

[//]: # (> ```yaml)

[//]: # (> data_cfg:)

[//]: # (>   dataset_name: sceneflow)

[//]: # (>   dataset_root:  your_path)

[//]: # (>   dataset_partition: ./datasets/sceneflow/CASIA-B_include_005.json)

[//]: # (>   num_workers: 1)

[//]: # (>   remove_no_gallery: false # Remove probe if no gallery for it)

[//]: # (>   test_dataset_name: sceneflow)

[//]: # (> ```)

[//]: # (> Now, suppose we get the model trained on [CASIA-B]&#40;http://www.cbsr.ia.ac.cn/english/Gait%20Databases.asp&#41;, and then we want to test it on [OUMVLP]&#40;http://www.am.sanken.osaka-u.ac.jp/BiometricDB/GaitMVLP.html&#41;.)

[//]: # (> )

[//]: # (> We should alter the `dataset_root`, `dataset_partition` and `test_dataset_name`, just like:)

[//]: # (> ```yaml)

[//]: # (> data_cfg:)

[//]: # (>   dataset_name: sceneflow)

[//]: # (>   dataset_root:  your_OUMVLP_path)

[//]: # (>   dataset_partition: ./datasets/kitti15/kitti15.json)

[//]: # (>   num_workers: 1)

[//]: # (>   remove_no_gallery: false # Remove probe if no gallery for it)

[//]: # (>   test_dataset_name: kitti15)

[//]: # (> ```)

[//]: # (---)

[//]: # (>)

[//]: # (<!-- ### Identification Function)

[//]: # (> Sometime, your test dataset may be neither the popular [sceneflow]&#40;http://www.cbsr.ia.ac.cn/english/Gait%20Databases.asp&#41; nor the largest [kitti15]&#40;http://www.am.sanken.osaka-u.ac.jp/BiometricDB/GaitMVLP.html&#41;. Meanwhile, you need to customize a special identification function to fit your dataset. )

[//]: # (> )

[//]: # (> * If your path structure is similar to [sceneflow]&#40;http://www.cbsr.ia.ac.cn/english/Gait%20Databases.asp&#41; &#40;the 3-flod style: `id-type-view`&#41;, we recommand you to  -->)

[//]: # ()
[//]: # (### Data Augmentation)

[//]: # (> In OpenGait, there is a basic transform class almost called by all the models, this is [BaseSilCuttingTransform]&#40;../opengait/data/transform.py#L20&#41;, which is used to cut the input silhouettes.)

[//]: # (>)

[//]: # (> Accordingly, by referring to this implementation, you can easily customize the data agumentation in just two steps:)

[//]: # (> * *Step1*: Define the transform function or class in [transform.py]&#40;../opengait/data/transform.py&#41;, and make sure it callable. The style of [torchvision.transforms]&#40;https://pytorch.org/vision/stable/_modules/torchvision/transforms/transforms.html&#41; is recommanded, and following shows a demo;)

[//]: # (>> ```python)

[//]: # (>> import torchvision.transforms as T)

[//]: # (>> class demo1&#40;&#41;:)

[//]: # (>>     def __init__&#40;self, args&#41;:)

[//]: # (>>         pass)

[//]: # (>>     )

[//]: # (>>     def __call__&#40;self, seqs&#41;:)

[//]: # (>>         ''')

[//]: # (>>             seqs: with dimension of [sequence, height, width])

[//]: # (>>         ''')

[//]: # (>>         pass)

[//]: # (>>         return seqs)

[//]: # (>> )

[//]: # (>> class demo2&#40;&#41;:)

[//]: # (>>     def __init__&#40;self, args&#41;:)

[//]: # (>>         pass)

[//]: # (>>     )

[//]: # (>>     def __call__&#40;self, seqs&#41;:)

[//]: # (>>         pass)

[//]: # (>>         return seqs)

[//]: # (>> )

[//]: # (>>  def TransformDemo&#40;base_args, demo1_args, demo2_args&#41;:)

[//]: # (>>     transform = T.Compose&#40;[)

[//]: # (>>         BaseSilCuttingTransform&#40;**base_args&#41;, )

[//]: # (>>         demo1&#40;args=demo1_args&#41;, )

[//]: # (>>         demo2&#40;args=demo2_args&#41;)

[//]: # (>>     ]&#41;)

[//]: # (>>     return transform)

[//]: # (>> ```)

[//]: # (> * *Step2*: Reset the [`transform`]&#40;../configs/baseline.yaml#L100&#41; arguments in your config file:)

[//]: # (>> ```yaml)

[//]: # (>> transform:)

[//]: # (>> - type: TransformDemo)

[//]: # (>>     base_args: {'img_w': 64})

[//]: # (>>     demo1_args: false)

[//]: # (>>     demo2_args: false)

[//]: # (>> ```)

[//]: # ()
[//]: # (### Visualization)

[//]: # (> To learn how does the model work, sometimes, you need to visualize the intermediate result.)

[//]: # (> )

[//]: # (> For this purpose, we have defined a built-in instantiation of [`torch.utils.tensorboard.SummaryWriter`]&#40;https://pytorch.org/docs/stable/tensorboard.html&#41;, that is [`self.msg_mgr.writer`]&#40;../opengait/utils/msg_manager.py#L24&#41;, to make sure you can log the middle information everywhere you want.)

[//]: # (> )

[//]: # (> Demo: if we want to visualize the output feature of [baseline's backbone]&#40;../opengait/modeling/models/baseline.py#L27&#41;, we could just insert the following codes at [baseline.py#L28]&#40;../opengait/modeling/models/baseline.py#L28&#41;:)

[//]: # (>> ```python)

[//]: # (>> summary_writer = self.msg_mgr.writer)

[//]: # (>> if torch.distributed.get_rank&#40;&#41; == 0 and self.is_train and self.iteration % 100==0:)

[//]: # (>>     summary_writer.add_video&#40;'outs', outs.mean&#40;2&#41;.unsqueeze&#40;2&#41;, self.iteration&#41;)

[//]: # (>> ```)

[//]: # (> Note that this example requires the [`moviepy`]&#40;https://github.com/Zulko/moviepy&#41; package, and hence you should run `pip install moviepy` first.)